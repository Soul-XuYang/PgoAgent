# 什么是矩阵？或者说矩阵的本质到底是什么？

### 前言
如果你是理工科的学生，矩阵是你一定不可避免碰到的一个数，它不仅仅是一个数学上的基本概念，其也广泛应用于其它学科，包括物理学、通信与信号处理、控制理论、计算机图形学、机器学习与数据分析等等。可以说它广泛运用于我们的日常生活之中。本节主要想探讨这样一个问题：什么是矩阵？或者说矩阵的本质到底是什么？
**注**:本文讨论的前提是读者对于矩阵有一点点了解,该文不会去讲解矩阵的加法、乘法或者是相关运算，重点是它的定义和概念。本文讨论的核心是谈谈这个如此具有数学对称美的一个东西——矩阵。
### 1.定义
在这里我引入了维基百科的定义：在数学上，一个 \(m \times n\) 的矩阵（英语：matrix）是一个有 \(m \times n\) 列（column）元素的矩形阵列。矩阵里的元素可以是数字或符号甚至是函数,具体见下图。
$$
\begin{bmatrix}a_{11}&a_{12}&a_{13}&\ldots&a_{1j}&\ldots&a_{1n}\\a_{21}&a_{22}&a_{23}&\ldots&a_{2j}&\ldots&a_{2n}\\a_{31}&a_{32}&a_{33}&\ldots&a_{3j}&\ldots&a_{3n}\\\vdots&\vdots&\vdots&\ddots&\vdots&\ddots&\vdots\\a_{i1}&a_{i2}&a_{i3}&\ldots&a_{ij}&\ldots&a_{in}\\\vdots&\vdots&\vdots&\ddots&\vdots&\ddots&\vdots\\a_{m1}&a_{m2}&a_{m3}&\ldots&a_{mj}&\ldots&a_{mn}\end{bmatrix}
$$
我自己的理解就是矩阵它本质是由一组元素按照矩形排列而成的数表，数表(矩阵)的元素可以是数字、符号或数学表达式。一般为了支持矩阵的运算，矩阵的元素之间应当能做加减法和乘法，最常见的是元素属于实数域或复数域的矩阵，简称为实矩阵和复矩阵，此外它也可以完全只有一行或者一列的形式，一般来说这种称之为向量，这与几何意义上的向量有密切关系。除此之外就没有什么特别之处。其实说实话到这里本文就可以结束了(当然肯定也没有这么简单QAQ)，矩阵其实从根本的数学形式来看就是一个数表，一个表格里面放着一些数字而已，或者更加简要概括的是就是一堆数字的集合形式。我自己的理解就是它本质就是一个代数工具，除此之外没有什么特别的，这就类似于数学里面的集合式或者函数表达式等等。
既然它是一个数表，那么请问这个数表有什么可以研究的？ 答：那肯定就是数表里的数据啊，比如这个表格里数有什么特点？数的分布有什么规律？或者是每一行或者每一列数据的关系或者这些数的乘积等等运算又代表了些什么？ 此外，矩阵和矩阵之间的关系又有哪些？它可以计算什么？而这些就是矩阵研究的内容或者说由此延伸出一门学科——矩阵理论
当然本文不会讲的这么深，首先是自己的水平肯定不够，其次是篇幅所限，再最后我自己也没有那么多时间，哈哈哈TvT！因而本文的主旨只是讨论下矩阵是什么？
### 2.矩阵由来
矩阵的概念并没有明确地出现在古代，但我们可以看到一些类似矩阵结构的思想。例如，在古埃及和古希腊的数学中，使用了类似的表格形式来表示方程组或数字的关系。通过这些初步的数学实践，当时的学者们意识到存在可以组织数字的结构。
矩阵作为一种形式化的数学对象的雏形，出现在18世纪末和19世纪初期，主要与线性方程组的求解密切相关。早期的数学家，特别是高斯(Carl Friedrich Gauss)和克莱姆(René Descartes)，发展了行列式和线性方程组的求解方法。例如，Gauss 的高斯消元法就是通过行变换来解线性方程组的一种方法。
比如用矩阵表示一个线性方程组
$$
\left\{\begin{aligned}
&1x_1+2x_2+3x_3+4x_4=5\\
&2x_1+7x_2+1x_3+8x_4=2\\
&x_1+6x_2+1x_3+8x_4=0\\
&6x_1+2x_2+8x_3+3x_4= 1\\
\end{aligned}\right.\\
{(一个线性方程组)}
$$$$
\left[\begin{array}{cccc|c}1&2&3&4&5\\2&7&1&8&2\\1&6&1&8&0\\6&2&8&3&1\end{array}\right]\\{(增广矩阵)}$$可以看到-左边4X4矩阵大小的是方程组系数，右边4X1矩阵大小的是方程组的运算结果。
然后用我们熟知的行变换来就可以求解出解的系数矩阵，当然也可以用克莱姆法则的行列式进行计算，最终结果如下：
\[
\left[\begin{array}{cccc|c}
1 & 0 & 0 & 0 & 1 \\
0 & 1 & 0 & 0 & -2 \\
0 & 0 & 1 & 0 & 3 \\
0 & 0 & 0 & 1 & -1
\end{array}\right]
\]可是上述的这种方式并不方便表示解，因而使用解向量这种的矩阵形式加以表示
\[
\mathbf{x} = \begin{pmatrix} 
x_1&=&1 \\ 
x_2&=&-2 \\ 
x_3&=&3 \\ 
x_4&=&-1 
\end{pmatrix}\\{(关于X的解向量)}
\]说实话学过同济那本线性代数教材的读者都应该知道，从历史角度来看：最开始矩阵和行列式是用来求解线性方程组的一个工具。但是该课本也只是单调地其给出定义和运算方式，然后就首先讲求解方程组，说实话我自己认为从方程组这块讲我认为它过抽象了，作为学生，我们只知道使用各种方式求解行列式的值并用它和矩阵来解线性方程组，这件事情是真的极其枯燥且乏味，完全沉溺于算算算并且也是用它来换一种形式求解个方程组，仅仅只是这种形式具有解线性方程组的普适性。这导致的我当时学习线性代数一点兴趣都没有，甚至我当时很抵触学习。有兴趣的童鞋可以看下3blue1brown有关线性代数的视频，这个真的是十分有趣。个人观点：从线性方程组为切入点学习，不能真正了解到矩阵到底是什么或者干什么，从向量的角度或者准确来说是空间变换的角度来说可以更加准确描述矩阵。
比如上述的求解方程组即是求解系数矩阵的一个**反变换**再乘以输出，且计算该变换的过程用到各种运算都是加减(可加性)和乘一个常数系数(齐次性)，因而其运算过程都是**线性运算**$$AX=b, X=A ^{-1}b$$因而，我在此给先给各位读者下一个结论，从数学意义上来讲：即一个矩阵就是表示一个**线性变换**，它里面的元素就是**该线性变换的一些参数**;或者也亦或者可以说是一个**数表**，其可用于存储数学元素，。 因而上述方程是描述的是对一个未知向量$X$进行$A$的**线性变换**换得到了$b$向量。
实际上，从形式来看，矩阵的使用只是表述了一个线性方程组这正是矩阵数表性质的表现即表示一组数据。
但同时上述$AX=b$的数学方程组首先可以将$X$看成一个向量,而$A$则是对于向量$X$的一个线性变换或者线性映射，从而得到另一个向量$b$.
### 3.矩阵的数表表达形式
#### 3.1矩阵数表
矩阵在各个领域上的的应用中其实十分常见，实际上从其定义来看，其本身就是一个数表的形式，因而我们常常离不开用矩阵表现一些数字组合。首先，上述第二章的例子已经陈述了，我们已经将线性方程组用矩阵加以表示其各个数据。再比如我们将二维图像数据上的点表示一个很庞大的数组，卷积核也用矩阵来加以表示。再比如，概率论上的协方差矩阵、多维的正太随机向量以及对应的正太过程的线性变换性质，都有其影子。接下来，我会以典型的微分方程和最小二乘法加以讲解。
#### 3.2 矩阵微分方程的扩展：从一维到多维
##### 3.2.1 标量（标量值）微分方程
在最基本的情况下，标量微分方程的形式是：
\[
\frac{dx(t)}{dt} = f(t, x(t))
\]
其中\(y(t)\)是一个标量函数，依赖于时间（或其他变量），并且它的导数与时间（或其他变量）及其自身的值有关。
这种形式的微分方程通常描述一个单一物理量的变化，比如物体的单一位置、温度的变化等。解这个方程，可以得到一个标量解 \(y(t)\)。
##### 3.2.2 矩阵（向量值）微分方程
当系统的状态变得更复杂时，问题通常不仅仅涉及单一的标量变量，而是一个多维的状态向量 \( \mathbf{x}(t) \)。在这种情况下，系统的动态行为可以通过向量微分方程来描述：
\[
\frac{d\mathbf{x}(t)}{dt} = A \mathbf{x}(t) + \mathbf{b}(t)
\]

其中：
- \( \mathbf{x}(t) \) 是一个 \(n \times 1\) 的列向量，表示系统的状态（例如**多个物理量**，如位置、速度、温度等）。
- \( A \) 是一个常数的 \( n \times n \) 矩阵，表示系统的动态特性（例如，描述系统各个变量之间的相互关系，或者系统的内部转换规律）。
- \( \mathbf{b}(t) \) 是外部输入或强迫项（例如，外部激励力）。
当我们从标量微分方程扩展到多维度微分方程时，最重要的变化就是状态从一个标量 \(y(t)\) 扩展为一个向量 \( \mathbf{x}(t) \)，并且矩阵 \(A\) 不再是一个标量常数，而是描述系统中各个变量之间相互作用的矩阵。
##### 3.2.3 矩阵指数与系统解
对于线性常微分方程的系统 \( \frac{d\mathbf{x}(t)}{dt} = A \mathbf{x}(t) \)，它的解可以写成矩阵指数的形式：

\[
\mathbf{x}(t) = e^{At} \mathbf{x}(0)
\]

这里，矩阵指数 \( e^{At} \) 描述了系统从初始状态 \( \mathbf{x}(0) \) 到时间 \( t \) 的演化过程。矩阵指数是一个自然的扩展，它类似于标量函数中指数的作用——表示某种增长或衰退的过程，但它能够处理多维度的状态向量。

##### 3.2.4 从标量到矩阵的扩展
上述所提到的扩展可以理解为从处理单个变量的动态演化，扩展到处理多个变量（多维状态空间）之间相互作用的动态系统。


- **标量微分方程**：描述单一物理量的演化，状态是单一的标量。
- **矩阵微分方程**：描述多个物理量或状态变量的联合演化，状态是多维向量或矩阵。

这种扩展的核心是：在多维度系统中，多个变量之间可能存在耦合关系，因此每个变量的变化不仅仅依赖于它自身的状态，还可能受到其他变量的影响。这种多维耦合的关系通常通过矩阵来表达。
以下将介绍一个实例加以展示：
##### 例子1：标量二阶微分方程
考虑一个简单的物理系统，比如**简单谐振子**的二阶微分方程：

\[
\frac{d^2y(t)}{dt^2} + \omega^2 y(t) = 0
\]

这个方程描述了一个质量-弹簧系统的位移 \(y(t)\) 随时间的变化。它是一个**二阶**的标量微分方程，表示了位移的加速度与位移本身之间的关系。

##### 当多个变量存在时，就会扩展到矩阵形式：变为二维系统
如果考虑一个**二维系统**（例如，两个耦合的质量-弹簧系统），状态变量就不仅仅是位移 \(y(t)\)，而是一个向量，包含了两个物理量的变化（比如位置和速度）：

\[
\frac{d}{dt} \begin{pmatrix} x_1(t) \\ x_2(t) \end{pmatrix} = \begin{pmatrix} 0 & 1 \\ -\omega^2 & 0 \end{pmatrix} \begin{pmatrix} x_1(t) \\ x_2(t) \end{pmatrix}
\]

这里：
- \(x_1(t)\) 表示位置，\(x_2(t)\) 表示速度。
- 矩阵 \(A = \begin{pmatrix} 0 & 1 \\ -\omega^2 & 0 \end{pmatrix}\) 描述了位置与速度之间的相互作用。

这个系统的解可以通过矩阵指数来得到：

\[
\begin{pmatrix} x_1(t) \\ x_2(t) \end{pmatrix} = e^{At} \begin{pmatrix} x_1(0) \\ x_2(0) \end{pmatrix}
\]
#### 3.3最小二乘法

**最小二乘法**的矩阵形式与我们之前讨论的矩阵微分方程有相似之处，都是将标量问题扩展到多维（矩阵或向量）空间的经典例子。最小二乘法的矩阵表示方法帮助我们更加高效地处理多元线性回归问题，尤其是在数据分析、机器学习等领域。

##### 3.3.1 最小二乘法的标量形式

最小二乘法（Least Squares）用于通过最小化误差平方和来拟合一个模型。在最简单的线性回归问题中，我们希望找到一个线性模型：

\[
y = \beta x + \epsilon
\]

其中：
- \( y \) 是观察值（目标变量），
- \( x \) 是自变量，
- \( \beta \) 是我们需要拟合的参数（斜率），
- \( \epsilon \) 是误差项。

对于多个观测值 \( (x_1, y_1), (x_2, y_2), \dots, (x_n, y_n) \)，最小二乘法的目标是最小化以下目标函数（误差的平方和）：

\[
J(\beta) = \sum_{i=1}^{n} \left( y_i - \beta x_i \right)^2
\]

然后，求导并求解最优的 \( \beta \) 值：

\[
\frac{dJ(\beta)}{d\beta} = 0 \quad \Rightarrow \quad \beta = \frac{\sum_{i=1}^{n} x_i y_i}{\sum_{i=1}^{n} x_i^2}
\]

这个标量形式的最小二乘法方法对于一个变量的情况非常直观，但当我们扩展到多个变量时（即多元线性回归），就需要引入矩阵的形式。

##### 3.3.2 最小二乘法的矩阵形式

在处理多个变量时，我们将问题推广到矩阵的形式。设有一个包含 \(n\) 个样本和 \(p\) 个特征（自变量）的数据集，记为矩阵 \( X \)：

\[
X = \begin{pmatrix}
x_{11} & x_{12} & \cdots & x_{1p} \\
x_{21} & x_{22} & \cdots & x_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n1} & x_{n2} & \cdots & x_{np}
\end{pmatrix}
\]

其中 \( X \) 是一个 \( n \times p \) 的矩阵，每一行是一个数据点，每一列是一个特征。目标是拟合一个 \( p \) 维的参数向量 \( \beta \)，使得预测值尽量接近真实观测值 \( y \)，其中 \( y \) 是一个 \( n \times 1 \) 的目标向量：

\[
y = X \beta + \epsilon
\]

##### 3.3.3 最小二乘法的优化目标

我们希望最小化目标函数（误差的平方和），即最小化以下表达式：

\[
J(\beta) = \|y - X\beta\|^2
\]

这个目标函数是一个二次型，表示观测值 \( y \) 与模型预测值 \( X \beta \) 之间的欧几里得距离。展开后，我们得到：

\[
J(\beta) = (y - X\beta)^\top (y - X\beta)
\]

##### 3.3.4. 求解最优参数 \( \beta \)

为了找到最优的 \( \beta \)，我们对 \( J(\beta) \) 求导并令其等于零：
\[
\frac{dJ(\beta)}{d\beta} = -2 X^\top (y - X \beta) = 0
\]
解得：

\[
X^\top X \beta = X^\top y
\]

这是一个**线性方程组**，其中 \( X^\top X \) 是一个 \( p \times p \) 的矩阵，通常称为**设计矩阵的Gram矩阵**，其解为：

\[
\beta = (X^\top X)^{-1} X^\top y
\]

这个公式就是最小二乘法的矩阵解。它给出了在多元线性回归中，如何通过矩阵运算得到最优参数 \( \beta \)。

#### 3.4 总结：矩阵形式的优势
将最小二乘法转化为矩阵形式有几个重要的优点：

- **通用性**：这种矩阵形式可以处理多个变量（特征）的情况，而不仅仅是一个变量。这在大数据分析和机器学习中非常重要。
- **高效计算**：矩阵运算可以高效地处理大量的数据，尤其是当数据维度很高时。现代计算机能够高效地进行矩阵运算（例如，使用优化的线性代数库）。
- **统一框架**：矩阵形式使得多种回归方法（如岭回归、LASSO等）能够统一在一个框架下进行推导，只需要对目标函数进行适当的调整。

在最小二乘法的矩阵形式和之前提到的矩阵微分方程之间，确实有一些类比：

- **扩展维度**：最小二乘法从一个简单的标量回归问题（单一自变量）扩展到多个自变量，类似于我们在矩阵微分方程中将一维的参数扩展到多维状态向量。
- **矩阵运算**：最小二乘法的核心是矩阵的运算，特别是矩阵的乘法、转置和求逆，这些运算在微分方程的求解中同样很重要。例如，矩阵指数在微分方程解的表示中起着类似的作用。
- **优化目标**：最小二乘法的优化过程（最小化误差平方和）与微分方程的解法有类似之处，都是通过一定的数学工具（如矩阵逆、特征值分解等）来寻找最优解。


**补充**:其实相关使用矩阵表示数字组合的例子很多，再比如随机过程里的马氏链的转移矩阵，不仅可以鲜明的表示各个状态的转移形式，还可以通过研究该矩阵对其进行分布和概率的研究，说实话这种例子实在是太多了。本文因为篇幅有限，好吧其实自己精力也有限^_^，就不在次进行过多举例和研究了。

### 四、空间的线性变换
个人认为，从空间的角度来说，更好的解释上述结论，在此是参考了3BlueB1Brown(一个我很喜欢的数学、物理以及计算机技术的科普博主)的线性代数的本质视频进行讲解的。
![alt text](/project1/3Blue1Brown.png)
#### 4.1 矩阵在空间上的线性变换
首先，我们以一个二维矩阵$A_1 = \begin{pmatrix} 2 & 2 \\ 2 & -2 \end{pmatrix}$为例子，我们以第一列的数据作为一个向量，它的维度即是行数，因此它包含两个坐标方向的数据，类似于XY直角坐标系的横坐标和纵坐标这两个方向，因此一个二维矩阵包含两个向量(这两个向量可以看成一个构成空间的一组基)，而每个基向量包含两个数据。下图(使用python编写)是上述矩阵$A_1$的空间基的空间表示形式,其中$ \alpha1是(2,2)^H,\alpha2是(2,-2)^H$，共两个基向量。
**注**图中黑色箭头表示变换前的向量，红色箭头表示变换后的向量。
![alt text](/project1/1Base_vector_matrix.png)
而实际上每一个矩阵表示的是一次空间向量的
旋转，该旋转的指的是以标准坐标基下的旋转，例如$A_2 = \begin{pmatrix} 1 & 3 \\ 2 & 4 \end{pmatrix}$，其指的是标准坐标基$ \alpha1(1,0)^H,\alpha2(0,1)^H $经过一个旋转矩阵旋转为$A_2$,因此它的作用就是将一个矩阵旋转对应矩阵坐标系下的$A_2$。例如，我以一个矩阵乘法为例子，下图为例，它展示了$A_1 \times A_2$的结果，其对应的是基$A_1$经过$A_2$线性变换后的结果，其中黑色向量是原始基向量，红色向量是线性变换后的向量。
![alt text](/project1/matrix_multiplication.png)
一个最基本的平面坐标系对应的基分别为$a_1=(1,0)^H,a_2=(0,1)^H$这两个，因而其对应的标准基向量矩阵为$\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$
比如说$B_1 = \begin{pmatrix} 4 & 0 \\ 0 & 4\end{pmatrix}$，对于一个对角矩阵来说，在空间上它表示的是两个维度方向的拉伸，该矩阵是第一维度对应的基向量拉伸四倍，第二维度对应维度的基向量拉伸4倍，
再比如说$B_2 = \begin{pmatrix} 5 & 0 & 0\\ 0 & 3 & 0 \\ 0 & 0& 2\end{pmatrix}$,它针对的是一个三维数据的数据变换，它将一个向量的第一维度数据扩大5倍,其第二维度扩大3倍，第三维度扩大2倍。第三维度扩大2倍。例如$\beta=(1,3,2)^H$,经过上述矩阵变换后，其变为$\beta=(5,9,4)^H$

由此，我们也可以引出一个有趣的矩阵——降维矩阵$C = \begin{pmatrix} 1 & 0 & 0\\ 0 & 1 & 0 \end{pmatrix}$,它会把第三维度的参量全部抹除而保留前两维度的数据，可以说是数学意义上的“二向箔”

接着我们在说酉矩阵或者实数域上我们也可称呼为正交矩阵，一个二维的酉矩阵可以为上述的标准正交基$\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$，因为酉变换(正交变换)的性质，其运算不改变结果的大小，故一个向量经过酉变换后大小不变，所以它改变的是方向，因而它的作用是使得向量发生旋转，
例如$D_1=\begin{pmatrix} -1 & 0 \\ 0 & -1 \end{pmatrix}$,其变换结果如下图
![alt text](旋转1.png)
再比如酉矩阵$D_2=\begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{-1}{\sqrt{2}} \end{pmatrix}$，它的旋转效果如下:
![alt text](旋转2.png)
当然随之，也产生了所需的旋转矩阵$\begin{pmatrix} \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{pmatrix}$,它是逆时针旋转$\theta$角度的矩阵，再比如三维上的一个矩阵
$ \begin{pmatrix} \cos(\theta) & -\sin(\theta) & 0\\ \sin(\theta) & \cos(\theta)&0 \\ 0 &0 &1\end{pmatrix}$，它指的是三维向量绕Z轴旋转，本质上就是Z轴数据不动，X和Y上正常旋转。

接下来会着重讲解矩阵的空间应用的实例分析。
##### 4.2 特征值和相似变换
1.特征变换指满足$Ax=\lambda x$,而$\lambda$是特征值，因而该矩阵变换表示的是对一个向量拉伸$\lambda$倍，因而它的空间意义是对于向量方向的拉伸，$\lambda$大于0代表正向拉伸，$\lambda$小于0代表反向拉伸。
2.而相似变换则是$P^{-1}AP=B$,它的意思是一个空间线性变换$B$等于从$P^{-1}$变换再经过$A$线性变换再经过$P$线性变换。

##### 4.3 SVD分解
SVD(Singular Value Decomposition，奇异值分解)是一种广泛应用于矩阵分析的分解方法，它将任意矩阵分解为三个矩阵的乘积，通常用于降维、图像处理、信号处理等领域。
比如，给定一个 $A_{m\times n}$ 的矩阵,我们可以通过SVD分解将其分解为三个矩阵乘积,第一个是酉矩阵$U_{m \times m}$，第二个是$\sum_{m \times n}$对角矩阵，第三个又是一个酉矩阵$V^H_{n\times n}$,其中$U^HU=I,V^HV=I$,而奇异值则是对角矩阵元素的非负元素的开根号值。
其中$U$是左奇异特征矩阵,它是$A^HA$的特征向量矩阵,而$V$是右奇异特征矩阵,它是$AA^H$的特征向量矩阵。其中二者的非零特征值是一样的，因而有共同的非零奇异值(该性质可以利用特征方程组加以证明)。
实际上，奇异值的大小$\delta$(其对应的是$\sum$里的对角线元素)对应的是矩阵的“重要性”，通常按从大到小排序，越大的奇异值对应矩阵的主成分，越小的奇异值则代表次要成分。实际上由于酉变换的范数不变性，我们可以从能量角度来看，奇异值的平方和表示了矩阵所蕴含的总能量。通过SVD降维，我们实际上是在选择保留最大的奇异值对应的主成分，来最大化数据的能量保留。如果一个矩阵的奇异值都很大，那么这个矩阵的能量较大，表示数据在这一方向上变换很快。见下两图,该图也是经典老图了,在这里说明下不知道该图出处是哪，只知道很多人都借此图讲解，望指正。
![alt text](svd1.png)
![alt text](svd2.png)
比如SVD实际的应用:例如,在SVD 在数据分析和机器学习中,它常用于降维(如主成分分析PCA)，通过保留较大的奇异值和相应的奇异向量，可以将数据映射到一个较低维度的空间，同时保持尽可能多的信息。而其中矩阵的秩是与奇异值的个数相关的，矩阵的秩等于其非零奇异值的数量。
而实际上我们也可以从空间变换加以理解，这里也是引用了网上的部分资料，实际上一个矩阵对应的一个空间变换可以分解为旋转+拉伸+旋转，向量方向的拉伸个数对应矩阵的秩。其中下图也是采用了网上的资料。
![alt text](svd-xuanzhuan.png)
![alt text](i-svd-xuanzhuan.png)
实际分析如下:假设矩阵 \( A \in \mathbb{R}^{m \times n} \)，它表示从 \( n \)-维空间到 \( m \)-维空间的一个线性变换。设有一个 \( n \)-维向量 \( \mathbf{x} \)，通过矩阵 \( A \) 进行变换，得到一个 \( m \)-维向量 \( \mathbf{y} \)：

\[
\mathbf{y} = A \mathbf{x}
\]

SVD 将矩阵 \( A \) 分解为三个部分：

\[
A = U \Sigma V^T
\]

其中：

- \( U \in \mathbb{R}^{m \times m} \) 是一个酉矩阵，包含矩阵 \( A \) 的行空间的正交基。
- \( \Sigma \in \mathbb{R}^{m \times n} \) 是一个对角矩阵，包含矩阵 \( A \) 的奇异值 \( \sigma_1, \sigma_2, \dots, \sigma_r \)（其中 \( r \) 是矩阵 \( A \) 的秩）。
- \( V^T \in \mathbb{R}^{n \times n} \) 是一个酉矩阵，包含矩阵 \( A \) 的列空间的正交基。

##### 变换步骤解析

##### 步骤 1：\( V^T \) 的作用 —— 从原始空间到新的坐标系

首先，\( V^T \) 对原始输入向量 \( \mathbf{x} \) 进行变换。这个操作的作用是将原始的 \( n \)-维空间中的向量通过一个酉变换（旋转或反射）转换到一个新的坐标系。在这个新的坐标系下，向量的分量表示为原始空间基的不同组合。

可以把 \( V^T \) 看作是把原始空间的基（向量）旋转到一组新的基，这组新的基对应的是矩阵 \( A \) 的列空间的正交基。换句话说，矩阵 \( A \) 的列向量所生成的空间被映射到一个标准正交基的空间。

##### 步骤 2：\( \Sigma \) 的作用 —— 按奇异值缩放

接下来，\( \Sigma \) 作用于变换后的向量。由于 \( \Sigma \) 是一个广义对角矩阵，它的对角线元素就是矩阵 \( A \) 的奇异值，记作 \( \sigma_1, \sigma_2, \dots, \sigma_r \)（其中 \( r \) 是矩阵 \( A \) 的秩，且 \( \sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_r \geq 0 \)）。

这一步的作用是对经过旋转后的向量进行缩放。每个向量分量的大小将会根据对应的奇异值被放大或压缩。如果某个奇异值为 0，那么对应的方向上的分量就会被压缩到零。

直观地说，\( \Sigma \) 的作用就是对向量在各个方向上的分量进行伸缩。奇异值越大，表示这个方向上的伸展越强；奇异值越小，表示该方向上的压缩越强。

##### 步骤 3：\( U \) 的作用 —— 将向量映射到输出空间

最后，矩阵 \( U \) 将变换后的向量映射到输出空间中。这个步骤可以看作是一个旋转或镜像的操作，它将经过缩放后的向量重新定位到 \( m \)-维空间中。矩阵 \( U \) 的列向量形成了矩阵 \( A \) 的行空间的正交基。因此，\( U \) 决定了输出空间的基，并且调整了输入向量在输出空间中的表示。


### 五、总结
在结束本文之前，我们要回答最后一个问题。为什么讨论矩阵运算这样问题的数学分支被称为线性代数？我们回顾一下矩阵和向量的乘法就知道答案了。
在运算时，左边的那个矩阵里的数字可以被看成是一组常数系数，右边竖着的向量中的数字则是未知数变量，这样矩阵和向量的乘法就变成了一组线性方程。如果把它们画在空间中，就是直线、平面或者立方体，都是线性的，不会有任何曲线。因此涉及到这一类的代数运算被称为线性代数。
我们今天讲了矩阵这个人们虚构出来的一种数学运算工具(一种抽象的数学概念)，利用这种工具，一是我们能够让计算从单个的一维度的，变成多维、可批处理的计算。要再次强调的是，将单个计算变成大批量处理，这是我们今天在信息时代要有的思维方式。二是矩阵其实表示的是线性变换即线性映射，它将数据通过一种线性变换映射到另一个数据，其本质上展现的是空间上的向量的变换。其实矩阵有多个性质和用途，我所讲的只是一个冰山一角，主要自己能力有限，目前能想到的就这么多了。
其实之所以想讨论矩阵就是因为想把这个东西搞清楚搞明白，满足下自身的好奇心。同时也想把这一个抽象的概念进行一个总结或者“大一统式”的解释吧，一个小小的期望和总结吧。
最后，感谢每一个可以认真看到这里的读者，这篇文章算是我写的最久的一篇了，以上既是我的分享，感谢各位的支持和鼓励。

